\documentclass[twoside]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{actes}
\usepackage[french]{babel}

\newcommand{\coq}{{\sc Coq}}
\newcommand{\ssr}{{\sc SSReflect}}

\title{ Formalisation des mathématiques : une preuve du théorème de Cayley-Hamilton $^*$ }

\author{Sidi Ould Biha $^1$}

\titlehead{Formalisation des mathématiques}%  a droite (page impaire)

\authorhead{Ould Biha}% a gauche (page paire)

\affiliation{\begin{tabular}{rr} 
\\ 1:  Inria de Sophia-Antipolis,
\\     2004, route des Lucioles - B.P. 93 06902 Sophia Antipolis Cedex, France
\\     {\tt Sidi.Ould\_biha@sophia.inria.fr} 
\\ $^*$:  Ce travail a été possible grâce au financement du laboratoire commun Microsoft-Inria
\\     {\tt http://www.msr-inria.inria.fr}
\end{tabular}}

\begin{document}
\setcounter{page}{1}
\maketitle

% Ici commence votre texte

\section{Introduction}
Les systèmes de preuves formelles peuvent être d'une grande utilité dans la vérification et la validation de preuves mathématiques, surtout lorsque ces preuves sont complexes et longues. Les travaux récents, comme la preuve formelle du théorème des 4 couleurs~\cite{4colproof} ou celle du théorème des nombres premiers~\cite{primeth}, montrent que ces systèmes ont atteint un niveau de maturité leur permettant de s'attaquer à des problèmes mathématiques non triviaux. Le travail de formalisation de preuves mathématiques faisant intervenir une large variété de structures mathématiques et nécessite l'adoption d'une approche semblable au génie logiciel. La formalisation de telles théories peut être vue comme un développement faisant intervenir différentes composantes : définitions et preuves mathématiques.
\paragraph*{}
Le théorème de Cayley-Hamilton, est l'un des théorèmes présents dans la liste des 100 théorèmes à formaliser~\cite{100th}. Ce papier en présente une première formalisation. Le fait qu'il n'avait pas été jusqu'à ce jour formalisé peut s'expliquer par le fait qu'il fait intervenir de nombreuses structures et propriétés mathématiques. Ces structures ne sont pas uniquement utilisées, de façon indépendante; mais elles sont aussi emboîtées les unes sur les autres. L'idée principale de notre travail est l'adoption d'une approche modulaire pour la formalisation des composantes utilisées dans la preuve, en particulier les polynômes et les matrices. Cette philosophie rentre dans le cadre de celle du projet ``Mathematical Components''~\cite{manif} dont l'un des objectifs est la formalisation du théorème de Feit-Thompson sur les groupes solvables. Dans ce cadre, le travail présenté dans cet article sera le point de départ du travail de formalisation de la théorie des caractères, une des composantes de la preuve du théorème de Feit-Thompson.
\paragraph*{}
L'article est organisé comme suit. Dans la section 2, nous présentons l'énoncé et la preuve du théorème de Cayley-Hamilton. Dans la section 3, nous présentons \ssr{}, l'extension de \coq{} et plate-forme de notre développement. Enfin, dans la section 4, nous présentons le développement qui a été nécessaire pour arriver à la formalisation du théorème de Cayley-Hamilton. 

\section{Le théorème de Cayley-Hamilton}
L'énoncé du théorème de Cayley-Hamilton~\cite{algebra} est le suivant : 
\begin{center}
 \textit{Toute matrice carrée sur un anneau commutatif annule son polynôme caractéristique.}
\end{center}
Plus formellement, soient $R$ un anneau commutatif et $A$ une matrice carrée sur $R$. Le polynôme caractéristique de $A$, défini par : $p_{A}(x) = \det{(xI_{n} - A)}$, s'annule en A; donc $p_{A}(A) = O_{n}$.
\newline
\paragraph{}
La preuve du théorème de Cayley-Hamilton~\cite{algebra} découle de la formule de Cramer selon laquelle la multiplication d'une matrice $B$ par la transposée de sa co-matrice est égale au déterminant de cette même matrice $B$ multiplié par la matrice identité :
\begin{equation}
  \label{Cramer}
 B * {}^t{{\rm com} B} = {}^t{{\rm com} B} * B =\det{B} * I_{n}
\end{equation}
En appliquant la formule (\ref{Cramer}) à la matrice $(xI_{n} - A)$ on obtient que :
\begin{equation}
  \label{Cramer-mx_poly-1}
 {}^t{{\rm com} (xI_{n} - A)} * (xI_{n} - A) =\det{(xI_{n} - A)} * I_{n}
\end{equation}
Dans la formule (\ref{Cramer-mx_poly-1}), la partie droite de l'égalité est égale au produit du polynôme caractéristique de $A$ par la matrice identité. La formule devient donc :
\begin{equation}
  \label{Cramer-mx_poly-2}
 {}^t{{\rm com} (xI_{n} - A)} * (xI_{n} - A) = p_{A}(x) * I_{n}
\end{equation}
Il est tentant de remplacer le $x$ par $A$ dans la formule (\ref{Cramer-mx_poly-2}). Dans la partie droite de l'égalité, nous obtiendrions $p_{A}(A) * I_{n}$, qui n'est autre que $p_{A}(A)$. Dans la partie gauche, la formule s'écrirait alors ${}^t{{\rm com} (AI_{n} - A)} * (AI_{n} - A)$, qui n'est autre que ${}^t{{\rm com} (AI_{n} - A)} * O_{n}$ ($O_{n}$ est la matrice nulle). En conclusion, nous obtiendrions que $p_{A}(A) = O_{n}$. En réalité, cette substitution n'est pas légale car dans (\ref{Cramer-mx_poly-2}) le $x$ est un scalaire et ne peut pas être substitué par $A$, qui est une matrice.
\newline
En fait, implicitement, la preuve classique du théorème utilise l'isomorphisme entre l'anneau des polynômes de matrices et celui des matrices de polynômes. En effet, toute matrice de polynômes peut s'écrire, de façon unique comme la somme de puissances en $x$ multipliées par des matrices, c'est-à-dire un polynôme à coefficients matriciels. Par exemple :
\begin{equation}
  \label{morphism}
 \left(
  \begin{array}{ c c }
     x^{2} + 1 & x - 2 \\
     - x + 3 & 2x - 4
  \end{array} \right)
=
 x^{2}\left(
  \begin{array}{ c c }
     1 & 0 \\
     0 & 0
  \end{array} \right) + 
x\left(
  \begin{array}{ c c }
     0 & 1 \\
     -1 & 2
  \end{array} \right) + 
\left(
  \begin{array}{ c c }
     1 & -2 \\
     3 & -4
  \end{array} \right)
\end{equation}
 Soit $\phi$ cet isomorphisme :
\[ \phi : M_{n}(R[x]) \rightarrow (M_{n}(R))[x] \]
Il est clair que l'image par $\phi$ de la matrice de polynôme $(xI_{n} - A)$ est le polynôme de matrice $(X - A)$. On utilise le $X$ pour différencier les polynômes de matrices de ceux sur l'anneau de base $R$. L'image par $\phi$ de la matrice de polynômes $p_{A}(x) I_{n}$ est le polynôme de matrices suivant :
\[ p_{A}(X) = a_{n} I_{n} X^{n} + a_{n-1} I_{n} X^{n-1} + \cdots + a_{1} I_{n} X + a_{0} I_{n}.\]
Dans la formule (\ref{Cramer-mx_poly-2}) les deux termes de l'égalité sont des matrices de polynômes. L'application de $\phi$ à ces termes nous donne l'égalité suivante :
   \[   \phi ( {}^t{{\rm com} (xI_{n} - A)} * (xI_{n} - A)) = \phi ( p_{A}(x) * I_{n})) \]
Les propriétés de morphisme de $\phi$ nous donnent alors que :
\[       \phi ( {}^t{{\rm com} (xI_{n} - A)}) * \phi ( xI_{n} - A)  = p_{A}(X) \]
Par construction de $\phi$ la formule ci-dessus s'écrit alors :
\[      \phi ( {}^t{{\rm com} (xI_{n} - A)}) * (X - A)  =  p_{A}(X) \]
Le polynôme $p_{A}(X)$ se factorise en $(X - A)$. Par le théorème du reste,  nous obtenons que $ p_{A}(A) = O_{n}$.
\paragraph*{}
Une formalisation de cette preuve du théorème de Cayley-Hamilton consiste à vérifier tous ses détails. Ce travail de formalisation n'est malheureusement pas aussi facile à écrire que l'énoncé du théorème ou sa preuve papier. En effet, et de manière générale, dans les preuves papiers plusieurs étapes sont implicites, ce qui ne facilite pas le travail de formalisation de telles preuves.
\paragraph*{}
L'énoncé du théorème de Cayley-Hamilton donne une relation entre les polynômes et les matrices, deux objets mathématiques différents. Pour pouvoir formaliser l'énoncé du théorème, il va falloir tout d'abord définir ces deux structures. Il est aussi question de polynôme caractéristique. La définition du polynôme caractéristique fait appel à celle de déterminant. En effet, le polynôme caractéristique est le déterminant d'une matrice de polynômes. La définition du déterminant nécessite à son tour celle des sommes et produits indexés, ainsi que celle des groupes de permutations.\newline
Le point départ de la preuve est la règle de Cramer. La formalisation de cette dernière nécessite la définitions des notions de co-matrice, de transposé et de déterminant, et la preuve de propriétés de multi-linéarités et de morphisme du déterminant. Dans la preuve, l'isomorphisme entre l'anneau des polynômes de matrices et celui des matrices de polynômes est présent de façon implicite. Lors du passage à la preuve formelle, cet isomorphisme devra être formalisé et ses propriétés de morphisme devront être prouvées. Les propriétés de morphisme de l'évaluation des polynômes sont utilisées par le théorème du reste. Il est aussi intéressant de noter que si dans l'énoncé il est question d'évaluation de polynôme de matrices, l'anneau des matrices n'est pas commutatif. Ainsi dans les propriétés de morphisme de l'évaluation, il a été nécessaire de considérer des hypothèses de commutativité entre les coefficients du polynôme évalué et la matrice que l'on évalue.
\paragraph*{}
En résumé, pour arriver donc à une formalisation et à la preuve du théorème de Cayley-Hamilton, il a fallu  développer une librairie sur les matrices (déterminant, formule de cramer, ...) et une autre sur les polynômes (morphisme d'évaluation, théorème du reste, ...). La formalisation de l'isomorphisme entre les matrices de polynômes et les polynômes de matrices sera l'étape ultime pour arriver à la preuve du théorème.  

\section{ \ssr }
\ssr~\cite{not4col,modgrp} (pour \textit{Small Scale Reflection} ou réflection à petite échelle) est une extension de \coq{}~\cite{coqart} qui introduit de nouvelles tactiques et des librairies \coq{} adaptées pour travailler sur des types avec une égalité décidable. Elle a été initialement développée par G. Gonthier dans le cadre de sa preuve du théorème des 4 couleurs. Un développement~\cite{modgrp} sur la théorie des groupes finis a été fait au dessus de \ssr. Ce développement comprend, entre autre, une formalisation du théorème de Sylow et du lemme de Cauchy-Frobenuis.\newline
Nous allons présenter en premier lieu la méthode de \ssr{} pour avoir la réflection entre les prédicats décidables et les booléens. Nous allons présenter par la suite le langage de tactiques introduit par \ssr{} et finalement nous présenterons les structures spécifiques de la librairie \ssr{} et que nous avons utilisées dans notre développement.
\subsection*{Réflection}
Dans le système de preuve \coq{} la logique par défaut est intuitionniste. Dans cette logique, les propositions logiques et les valeurs booléennes sont distinctes. Lorsque nous travaillons avec des types décidables cette distinction n'a pas lieu d'être. \ssr{} permet de combiner le meilleur des deux visions et de passer de la version propositionnelle d'un prédicat décidable vers la version booléenne. Pour ce faire, le type booléen est injecté dans celui des propositions par une coercion : 
\begin{verbatim}
Coercion is_true (b: bool) := b = true.
\end{verbatim}
Ainsi, et de façon transparente pour l'utilisateur, lorsque \coq{} attend un objet de type \texttt{Prop} et reçoit une valeur \texttt{b} de type \texttt{bool}, il la traduira automatiquement en la proposition \texttt{(is\_true b)}, qui correspond à la proposition \texttt{b $=$ true}.\newline
Le prédicat inductif \texttt{reflect} permet d'avoir une équivalence entre les propositions décidables et les booléens :
\begin{verbatim}
Inductive reflect (P: Prop): bool -> Type :=
  | Reflect_true: P => reflect P true
  | Reflect_false: ~P => reflect P false.
\end{verbatim}

La proposition \texttt{(reflect P b)} indique que \texttt{P} est équivalent à \texttt{(is\_true b)}. Par exemple, l'équivalence entre la conjonction booléenne \texttt{\&\&} et celle propositionnelle \verb|/\| est donnée par le lemme suivant : 
\begin{verbatim}
Lemma andP: forall a b, reflect (a /\ b) (a && b).
\end{verbatim}
Dans ce lemme \texttt{a} et \texttt{b} sont des variables booléennes.

\subsection*{Langage de tactiques}
Les scripts de preuve écrits avec \ssr{} diffèrent de ceux écrits dans \coq{} standard. \ssr{} ajoute une nouvelle couche au langage de tactique de \coq{}. En pratique, les scripts de preuve écrits avec \ssr{} se révèlent plus concis que ceux écrits dans \coq{} standard. \newline
Toutes les opérations fréquentes qui consistent à déplacer ou généraliser depuis ou vers le contexte courant des formules sont regroupées dans la tactique \verb|move|. Par exemple la tactique ``\verb|move: (H1 a)|''  permet de placer dans le but courant une instance de l'hypothèse \verb|H1| pour la variable \verb|a|. Un autre exemple est la tactique ``\verb|move=> x y H2|'' qui correspond à l'introduction des variables \verb|x| et \verb|y|, et d'une nouvelle hypothèse \verb|H2| dans le contexte courant. La tactique \verb|move: (H1 a) => H2 x y| correspond à la combinaison des deux exemples précédents dans une seule et unique tactique.\newline
La tactique \verb|rewrite| permet de combiner toutes les opérations de réécriture conditionnelle, de dépliage de définition, de simplification et de réécriture pour une occurrence ou un pattern donné. Ces opérations peuvent être utilisées ensemble ou séparément. Par exemple la tactique \verb|rewrite /def H1 ?H2 !H3| permet de déplier la définition \verb|def|, de récrire avec l'hypothèse \verb|H1|, de récrire zéro ou plusieurs fois avec l'hypothèse \verb|H2|, et de récrire au moins une fois avec l'hypothèse \verb|H3|. Un autre exemple est celui de la tactique \verb|rewrite {2}[_ * _]H4 //| qui permet de récrire dans la seconde occurrence du pattern \verb|[_ * _]| avec l'hypothèse \verb|H4| et de simplifier le but courant.\newline
Le mécanisme de réflection entre les propositions décidables et les booléens décrit plus haut est intégré au nouveau langage de tactique. Par exemple, étant donné un contexte avec une proposition \verb|H| de type \verb|a && b|, la tactique \verb|move/andP : H => H| applique le lemme \verb|andP| à \verb|H| et introduit dans le contexte une hypothèse \verb|H| de type \verb|a /\ b|. En revanche, lorsque le but est de la forme \verb|a && b|, la tactique \verb|apply/andP| change le but par \verb|a /\ b|. Enfin, lorsque le but est de la forme \verb|(a && b) -> G|, la tactique \verb|case/andP => H1 H2| change le but en \verb|G| et introduit deux hypothèses \verb|H1 : a| et \verb|H2 : b|.


\subsection*{Librairies}
Des librairies de base sont définies dans \ssr. C'est une hiérarchie de structure pour travailler avec les types décidables et en particulier les types finis. La structure \verb|eqType| définit les types munis d'une égalité décidable.
\begin{verbatim}
Structure eqType : Type := EqType {
  sort :> Type; 
  _ == _ :  sort -> sort -> bool; 
  eqP :  forall x y, reflect (x = y) (x == y)
}.
\end{verbatim} 
Le symbole \verb|: >|  déclare \verb|sort| comme une coercion d'un \verb|eqType| vers son type porteur. C'est la technique standard de sous-typage, toute objet de type \verb|eqType| est aussi de type \verb|Type|. La structure \verb|eqType| ne suppose pas seulement l'existence d'une égalité décidable \verb|==|, en plus elle injecte cette égalité vers celle de Leibniz. \newline
Une propriété majeure des structures d'\verb|eqType| est qu'elles donnent la propriété de la \textit{proof-irrelevance} pour les preuves d'égalités de leurs éléments. Ainsi il n'y a qu'une seule preuve de l'égalité pour chaque paire d'objets égaux.
\begin{verbatim}
Lemma eq_irrelevance: forall (d: eqType) (x y: d) (E: x = y) (E': x = y), E = E'.
\end{verbatim} 
\paragraph*{}
Un ensemble sur une structure d'\verb|eqType| est représenté par sa fonction caractéristique : 
\begin{verbatim}
Definition set (d: eqType) := d -> bool.
\end{verbatim} 
Avec cette définition, les opérations ensemblistes comme l'intersection ou l'union se définissent avec les fonctions booléennes correspondantes.
\paragraph*{}
Le type des listes sur un \verb|eqType d| se définit de façon inductive par :
\begin{verbatim}
Inductive seq : Type := Seq0 | Adds (x : d) (s : seq).
\end{verbatim} 
\verb|Adds| et \verb|Seq0| correspondent respectivement aux constructeurs \verb|cons| et \verb|nil| du type standard \verb|list| de \coq. Le type \verb|seq d| définit les listes sur un \verb|eqType d|. La décidabilité de l'égalité sur \verb|d| permet de définir les opérations d'appartenance et de recherche d'occurrence dans une liste. L'opération d'appartenance à une liste va permettre la définition d'une coercion de liste vers \verb|set|. Pour une liste \verb|l, (l x)| peut être représentée par \verb|x| $\in$ \verb|l|.\newline
Une fonction utile sur les séquences est la fonction \verb|filter|. Elle prend en paramètre un ensemble \verb|a| et une séquence \verb|s| et ne retourne que les éléments de \verb|s| qui appartiennent à \verb|a|. L'opération d'extraction d'un élément d'une liste est donnée par la fonction \verb|sub|. Par exemple \verb|sub x0 s i| retourne l'élément d'indice \texttt{i} de la séquence \texttt{s}, si \texttt{i} est strictement inférieure à la longueur de la séquence, et \texttt{x0} dans le cas contraire.La fonction \verb|foldr| correspond à l'opération fold utilisée en programmation fonctionelle. Elle est définie par récurrence sur une séquence :
\begin{verbatim}
Variables (d : eqType) (R : Type) (f : d -> R -> R) (z0 : R).
Fixpoint foldr (s : seq d) : R :=
  match s with | Seq0 => z0 | Adds x s' => f x (foldr s') end.
\end{verbatim} 

\paragraph*{}
La définition du type liste sur un \verb|eqType| est à la base de celle des types finis. La structure \verb|finType| se compose d'une liste sur un \verb|eqType| et de la preuve qu'aucun élément de cette liste n'apparaît plus d'une fois.
\begin{verbatim}
Structure finType : Type := FinType {
  sort :> eqType;
  enum :  seq  sort;
  enumP :  forall x, count (set1 x) enum = 1
}.
\end{verbatim} 
Dans cette définition \verb|(set1 x)| est l'ensemble singleton ${x}$ et \verb|(count f l)| calcule le nombre d'éléments \verb|y| de la liste \verb|l| pour lesquels \verb|(f y)| est vraie. Le paramètre \verb|enum| correspond à la liste des éléments du type fini. Par exemple pour un \verb|finType d|, (\verb|enum d|) retourne la liste des éléments de \verb|d|. \newline
Pour représenter les types finis à \(n\) éléments, la bibliothèque \ssr{} fournit une famille de types nommée \verb|ordinal| dont les éléments sont des paires composées d'un nombre entier \(p\) et d'une preuve que \(p\) est inférieur à \(n\). Comme cette preuve est basée sur un test décidable, la propriété d'irrelevance s'applique et les éléments de ce type sont uniquement caractérisés par la composante \(p\). La notation \verb|I_(n)| désigne le type \verb|ordinal n|.

\paragraph*{}
Dans le système \coq, l'axiome d'extensionnalité n'est pas admis par défaut. En effet lorsque deux fonctions \texttt{f} et \texttt{g} sont égales sur tous les éléments de leurs domaines ($\mathtt{\forall x, f~x = g~x}$ ce qui correspond à la notation \texttt{f =1 g}), ceci n'implique pas que \texttt{f = g}. Lorsque les fonction sont à deux arguments la notation \texttt{f =2 g} correspond à la proposition ($\mathtt{\forall x~y, f~x~y = g~x~y}$). Dans le cas où les domaines des fonctions manipulées sont finies et les co-domaines sont des eqType, elles peuvent être représentées par leur graphe et ainsi nous obtenons une égalité de Leibniz sur ces fonctions sans avoir besoin d'ajouter d'axiomes. Etant donnés un type fini \texttt{d1} et un type \texttt{d2} muni d'une égalité décidable, un graphe est défini par :
\begin{verbatim}
Inductive fgraphType : Type := 
  Fgraph (val: seq d2) (fgraph_sizeP: size val = card d1): fgraphType.
\end{verbatim} 
Le type contient une liste \texttt{val} d'éléments de \texttt{d2} et la preuve que la taille de cette liste est égale au cardinal de \texttt{d1}. Une structure de eqType et une autre de finType sont définies sur les graphes de fonction. Elles sont respectivement représentées par \verb|fgraph_eqType| et \verb|fgraph_finType|. La fonction \verb|fgraph_of_fun| permet de calculer le graphe d'une fonction donnée alors que la fonction \verb|fun_of_fgraph| permet de voire le graphe comme une fonction. Grâce à ces définitions, l'extensionnalité fonctionnelle peut être prouvée : 
\begin{verbatim}
Lemma fgraphP : forall (f g : fgraphType d1 d2), f =1 g <=> f = g.
\end{verbatim}
Il est à noter que dans la partie gauche de l'équivalence, \texttt{f =1 g} correspond en réalité à \texttt{(fun\_of\_fgraph f) =1 (fun\_of\_fgraph g)}. En effet \texttt{fun\_of\_fgraph} est une coercion entre \texttt{fgraphType} et le type des fonctions.

\section{Formalisations \coq}
\subsection{Les opérations indexées} 
Dans la définition des opérations sur les matrices, par exemple la multiplication ou le calcul du déterminant, les opérations indexées (somme et produit) sont fréquentes. Factoriser la preuve de propriétés générales sur les sommes et produits indexés permet de réduire considérablement la longueur et la complexité des preuves.  Une librairie pour les opérations indexées n'est pas seulement utile dans le développement sur la théorie des matrices, elle pourra l'être aussi dans les développements sur l'algèbre linéaire.
\paragraph{}
Une opération indexée est l'application d'une opération binaire aux éléments d'une suite indexée par un ensemble fini. C'est une généralisation de la définition d'une opération binaire en une opération n-aire.
Dans le cas particulier de l'addition, c'est la somme de tous les éléments d'une suite donnée. \newline
La fonction qui généralise la définition d'une opération binaire \verb|op| sur un type \verb|R| en une opération n-aire est la suivante :
\begin{verbatim}
Definition iop (d : finType)(a : set d)(f : d->R) :=
  foldr (fun x => op (f x)) nil (filter a (enum d)).
\end{verbatim}
Pour utiliser cette fonction, on fixe d'abord l'opération binaire \verb|op| et la valeur \verb|nil| qui sera associée aux ensembles vides. Dans ce contexte \verb|iop d a f| représente :
\[ f ~ a_{1} ~ op ~ f ~ a_{2} ~ op ~ \cdots ~ op ~ f ~ a_{n} ~ op ~ nil,  \] 
lorsque les $a_{i}$ sont tous les éléments de l'ensemble $a$. L'utilisation de \verb|iop| est plus naturelle lorsque l'opération est associative et commutative et lorsque \verb|nil| est l'élément neutre de cette opération.

\paragraph*{}
Donnons quelques lemmes intéressants sur {\tt iop}.  Le premier permet de changer le type des indices de l'opération. Dans le cas d'une somme indexée, ce lemme correspond à l'égalité entre les sommes 
$ \sum_{i=0}^{n} (i + m)$ et $ \sum_{j=m}^{n + m} j $. Une façon de formaliser cette égalité est de considérer que $i$ et $j$ sont de types différents et qu'il existe une bijection locale entre ces types.
Cette bijection est la fonction $ f : x \leftarrow x + m $, qui est localement bijective sur l'interval $ [0..n] $. \newline
Le prédicat \verb|ibjective r h| permet de dire que la fonction \verb|h| de type \verb|d' -> d| est localement bijective sur l'ensemble {\tt r}. Le lemme de ré-indexation s'énonce alors comme suit : 
\begin{verbatim}
Lemma reindex_iop :
  forall (d d' : finType) (h : d' -> d) r (f :d->R),
   ibijective r h ->
    iop d r f = iop d' (fun i => r (h i)) (fun i=> f (h i)).
\end{verbatim}

Un autre résultat intéressant sur les opérations indexées est celui qui permet de décomposer cette opération suivant une partition de l'ensemble d'indice. Par exemple, dans le cas d'une somme indexée, le résultat s'écrit $ \sum_{i=0}^{n + m} i = \sum_{i=0}^{n} i + \sum_{i=n+1}^{n + m} i $. La généralisation de cette propriété peut s'écrire formellement :
\begin{verbatim}
Lemma iopID :
  forall (d : finType)(c r : set d) f,
   iop d r f = 
   op (iop d (fun i => r i && c i) f) 
    (iop d (fun i => r i && ~~ c i) f).
\end{verbatim}
Dans ce lemme, étant donnés deux ensembles \texttt{c} et \texttt{r}, une partition de \texttt{r} est donnée par les deux ensembles $\mathtt{r \cap c}$ et $\mathtt{r \cap \bar{c} }$. La somme des éléments indexés par \texttt{r} peut être donc décomposée en deux sommes des éléments indexés par ces deux ensembles. 
\paragraph*{}
Bien sûr, ces deux lemmes ne sont prouvés que dans le cas où l'opération {\tt op} est commutative, associative et la valeur {\tt nil} est élément neutre.

\subsection{Structures canoniques}
Le théorème de Cayley-Hamilton parle de polynômes de matrices et de matrices sur des anneaux commutatifs. Les structures algébriques utilisées dans la preuve sont toutes des structures d'anneaux. Les anneaux sont définis par une structure qui regroupe les propriétés standards d'anneau.
\begin{verbatim}
Structure ring : Type := Ring {
  element :> eqType;
  0 : element;
  1 : element;
  - _ : element -> element;
  _ + _ : element -> element -> element;
  _ * _ : element -> element -> element;
  zeroP : forall x, 0 + x = x;
  oppP : forall x, - x + x = 0;
  plusA : forall x y z, x + (y + z) = (x + y) + z;
  plusC : forall x y, x + y = y + x;
  onePl : forall x, 1 * x = x;
  onePr : forall x, x * 1 = x;
  multA : forall x y z, x * (y * z) = (x * y) * z;
  plus_mult_l: forall x y z, x * (y + z) = (x * y) + (x * z);
  plus_mult_r: forall x y z, (x + y) * z = (x * z) + (y * z);
  one_diff_zero_ : 1 <> 0 (* exclure l'anneau trivial *)
}.
\end{verbatim} 
Il est à remarquer que dans la structure ring ne contient pas la propriété de commutativité de la multiplication de l'anneau. Si l'anneau des matrices n'est pas commutatif, celui des polynômes peut être commutatif si l'anneau de ses coefficients l'est aussi. Dans la preuve, d'une part la règle de Cramer sur les matrices n'est vraie que si l'anneau des coefficients de la matrice est commutatif. Pour appliquer Cramer sur l'anneau des matrices de polynômes, il faut que ce dernier soit commutatif, ce qui nécessite que l'anneau des coefficients soit à son tour commutatif. D'autre part, les polynômes de matrices sont construits sur l'anneau des matrices, qui n'est pas commutatif. L'anneau des polynômes peut se définir sur une structure d'anneau commutatif ou non commutatif. \newline
La structure d'anneau commutatif est définie au dessus de celle d'anneau en lui ajoutant la propriété de commutativité de son opération de multiplication.
\begin{verbatim}
Structure commutative_ring : Type := CommutativeRing {
  element :> ring;
  multC :  forall x y, x * y = y * x
}.
\end{verbatim} 
Le système de coercion de \coq{} permet à la structure d'anneau commutatif d'hériter de celle d'anneau simple tous les lemme standards.
\paragraph*{}
Grâce à ces définitions, les structures d'anneau de polynômes et de matrices se construisent sur celle d'anneau. Les constructeurs de ces structures prennent en paramètre un anneau et retournent l'anneau correspondant. Ainsi, pour définir les polynômes de matrices il suffira de passer comme paramètre au constructeur de la structure de polynômes l'anneau des matrices. La même méthode s'applique dans le sens inverse pour définir les matrices de polynômes. \newline
Un autre avantage est d'avoir la même notation pour les opérations d'anneau (addition, multiplication et opposé) sur les polynômes et les matrices. Ceci rend l'énoncé de nos théorèmes très proches des notations utilisées en mathématiques classiques où il n'y a pas de différence entre l'addition de polynômes ou celle de matrices. Ceci se fait à l'aide du mécanisme de \texttt{Canonical Structure}~\cite{coqman} de \coq. Un exemple d'utilisation de ce mécanisme dans l'exemple des matrices est présenté dans la prochaine section.

\subsection{Matrices et déterminants}
Une matrice sur un anneau $R$ est une séquence de coefficients doublement indexée. Elle peut être vue comme une fonction qui associe à une position  \((i,j)\) une valeur dans l'anneau $R$. Étant donnés $m$ et $n$ deux entiers et $R$ un anneau, une matrice sur $R$ (un objet de type $M_{m,n}(R)$) peut être représentée par la fonction suivante : $ [0..n[ \rightarrow [0..m[ \rightarrow R$. Le type des matrices de taille \((m,n)\) est défini par :
\begin{verbatim}
Definition matrix (m n :nat) :=
  fgraph_eqType (I_(m) * I_(n)) R.
\end{verbatim} 
Dans cette définition, les matrices sont des fonctions à deux arguments. Nous avons donc défini les fonctions \verb|matrix_of_fun| et \verb|fun_of_matrix| qui permettent respectivement de définir un objet de type \texttt{matrix} à partir d'une fonction et de convertir un objet de type \texttt{matrix} en une fonction à deux arguments. Cette dernière n'est autre qu'une coercion du type \texttt{matrix} vers celui des fonctions. Elle nous permet de dire que deux matrices A et B sont égales si et seulement si nous avons \texttt{A =2 B}. Ce qui veut dire que les fonctions associées à ces matrices sont égales.\newline
Dans la suite, les notations \texttt{M\_(n)}, \texttt{+m}, \texttt{*m}, \texttt{*sm}, \verb|\0m| et \verb|\1m| correspondent respectivement au type des matrices carrées, à l'addition de deux matrices, la multiplication de deux matrices, la multiplication d'une matrice par un scalaire, la matrice nulle et la matrice unité. 
\paragraph*{}
Après avoir défini le type des matrices et pour construire la structure d'anneau sur le type des matrices carrées, il va falloir prouver les axiomes d'anneaux sur ce type. 
\begin{verbatim}
Variable (n : nat) (Hn : 0<n).
Lemma mx_plus0x: forall (A : M_(n)), \0m +m A = A.
Lemma mx_scale_oppl: forall (A : M_(n)), (- 1 *sm A) +m A = \0m.
Lemma mx_plusA: forall (A B C : M_(n)), A +m (B +m C) = (A +m B) +m C.
Lemma mx_plusC: forall (A B : M_(n)), A +m B = B +m A.
Lemma mx_mult1x: forall (A : M_(n)), \1m *m A = A.
Lemma mx_multx1: forall (A : M_(n)), A *m \1m = A.
Lemma mx_multA: forall (A B C : M_(n)), A *m (B *m C) = (A *m B) *m C.
Lemma mx_distrL: forall (A B C : M_(n)), A *m (B +m C) = (A *m B) +m (A *m C).
Lemma mx_distrR: forall (A B C : M_(n)), (A +m B) *m C = (A *m C) +m (B *m C).
Lemma mx_0_diff_1: \0m <> \1m.
Canonical Structure matrix_ring: ring:=
  Ring mx_plus0x m_scale_oppl mx_plusA mx_plusC mx_mult1x mx_multx1
       mx_multA mx_distrL mx_distrR mx_0_diff_1.
\end{verbatim} 
L'utilisation de \texttt{Canonical Structure} dans la dernière déclaration permettra à Coq d'inférer automatiquement la structure d'anneau de matrice lorsque nécessaire. C'est par exemple le cas dans le lemme suivant qui dit que la trace de la somme de deux matrice est la somme des traces des deux matrices. 
\begin{verbatim}
Lemma trace_plus_mx : forall n (A B : M_(n)), \tr (A + B) = \tr A + \tr B.
\end{verbatim} 
Dans l'énoncé du lemme les matrices \texttt{A} et \texttt{B} ne sont pas déclarées de type \texttt{matrix\_ring} mais simplement de type \texttt{matrix}, \coq{} a inféré automatiquement la structure d'anneau et a accepté l'utilisation de la notation pour l'addition d'anneau dans la partie gauche de l'égalité.
\paragraph*{}
Pour définir les déterminants, nous avons utilisé la formule de Leibniz où nous supposons que $A$ 
est une matrice carrée de dimension $n$.
\begin{equation}
  \label{leibniz} \det(A)=\sum_{\sigma \in S_n} 
\epsilon(\sigma) \prod_{i=1}^n a_{i, \sigma(i)}
\end{equation} 
Dans cette formule, nous utilisons des opérations indexées pour les sommes et les produits, mais les notations mathématiques cachent plusieurs autres éléments qui doivent être formalisés précisément:
\begin{itemize}
 \item l'indexation des lignes et colonnes de la matrice par des entiers est remplacée par  une 
indexation par les éléments du type \texttt{I\_(n)},
 \item l'ensemble des permutations sur cet ensemble fini doit également être décrit comme un 
 ensemble fini qui pourra être énuméré,
 \item pour toute permutation il est nécessaire de calculer sa parité.
\end{itemize}

\paragraph{}
Pour représenter les permutations, la bibliothèque \ssr{} utilise les \texttt{fgraphType}. Le type permutation est facilement décrit comme un type fini qui peut être énuméré et sur lequel on peut faire des opérations indexées finies. Pour calculer la parité d'une permutation \(\sigma\), on veut compter le nombre d'inversions de la permutation, c'est à dire l'ensemble des paires \((i,j)\) telles que \(i < j\)  et \(\sigma(j) < \sigma(i)\). Pour ce travail, nous utilisons une théorie des multiplets sur des types finis, un type fini et l'ordre naturel des éléments induits par l'ordre dans la séquence \texttt{enum} qui définit ce type. Cet ordre est utilisé pour comparer \(i\) et \(j\) d'une part et \(\sigma(i)\) et \(\sigma(j)\) d'autre part.
\paragraph{}
Grâce à ces développements la formule (\ref{leibniz}) peut alors s'écrire :
\begin{verbatim}
 Definition determinant n (A : M_(n)) :=
  \sum_(s : S_(n)) (-1) ^ s * \prod_(i) A i (s i).
\end{verbatim} 

Les notations \verb|\sum| et \verb|\prod| représentent respectivement la somme et le produit indexées. Ce sont des instances de \verb|iop| pour les opérations internes (addition et multiplication) de l'anneau des coefficients de la matrice. La notation \verb|S_(n)| représente le groupe des permutations de taille $n$. Dans la suite, la notation \verb|\det| représentera la fonction \verb|determinant|.

\paragraph{}
Pour montrer que le déterminant est une forme linéaire, nous avons eu besoin de montrer que la structure 
d'anneau sur l'ensemble des coefficients où la multiplication est associative, l'addition est commutative, 
et la multiplication distributive sur l'addition se retrouve sur les opérations indexées. Ces preuves se 
font assez simplement. Nous arrivons donc au point où nous voulons exprimer des relations entre les lignes de plusieurs matrices. Pour cela nous utilisons deux fonctions : la fonction {\tt row} prend en entrée un nombre \(i\) inférieur à \(m\) (un élément de type {\tt I\_(m)}) et une matrice \((m,n)\); elle retourne la matrice \((1,n)\) (une rangée et \(n\) colonnes) qui contient la rangée \(i\).  Avec les mêmes arguments la fonction {\tt row'} retourne la matrice \((m-1,n)\) où la rangée choisie a été enlevée.  Grâce à ces fonctions la multilinéarité s'écrit de la façon suivante:
\begin{verbatim}
 Lemma determinant_multilinear : forall n (A B C : M_(n)) i b c,
    row i A =2 b *sm row i B +m c *sm row i C ->
    row' i B =2 row' i A -> row' i C =2 row' i A ->
  \det A = b * \det B + c * \det C.
\end{verbatim} 
Nous devons également démontrer que le déterminant est une forme alternée. Pour cela il est nécessaire de montrer plusieurs propriétés sur la parité des permutations. En particulier, nous avons établi que pour la
composition de toute permutation avec une transposition entre deux éléments distincts établit une bijection entre les permutations paires et les permutations impaires. Les lemmes de partitionnement et de réindexation des opérations indexées suffisent alors pour obtenir l'énoncé suivant:
\begin{verbatim}
Lemma alternate_determinant : forall n (A : M_(n)) i1 i2,
  i1 != i2 -> row i1 A =1 row i2 A -> \det A = 0.
\end{verbatim}
Des calculs dans le même esprit permettent d'obtenir la propriété de morphisme du déterminant :
\begin{verbatim}
Lemma determinantM :
 forall n (A B : M_(n)), \det (A * B) = \det A * \det B.
\end{verbatim}
Puis nous pouvons définir la co-matrice d'une matrice à l'aide de la fonction {\tt row'} et d'une fonction similaire sur les colonnes, puis la transposée de cette co-matrice (représentée par la fonction {\tt adjugate}) et prouver l'égalité de Cramer :
\begin{verbatim}
Lemma mult_adugateR :
 forall n (A : M_(n)), A * adjugate A = \det A *sm \1m.
\end{verbatim}

\subsection{Polynômes}
Un polynôme est défini par la liste de ses coefficients $a_{i}$ qui appartiennent à un anneau $R$: 
\[ a_{n} x^n + a_{n - 1} x^{n - 1} + \cdots + a_{1} x + a_{0}\]
Cette représentation n'est malheureusement pas unique, en effet les polynômes $ a_{n} x^n + a_{n - 1} x^{n - 1} + \cdots + a_{1} x + a_{0} $ et $ 0 x^{n + 1} + a_{n} x^n + a_{n - 1} x^{n - 1} + \cdots + a_{1} x + a_{0} $ ont des listes de coefficients différentes mais représentent le même polynôme. Pour avoir une égalité de Leibniz pour cette représentation, il est nécessaire de ne considérer que les polynômes normalisés, c'est-à-dire ceux dont le coefficient de plus grand degré est non nul. Les polynômes sont donc représentés par la structure suivante :
\begin{verbatim}
Structure polynomial (R : ring) : Type := Poly {
  p :>  seq R;
  normal : last 1 p != 0
}.
\end{verbatim} 
Avec cette définition nous pouvons donc définir une structure de \verb|eqType| sur les polynômes. Mais il est parfois utile de voir les polynômes juste comme une fonction non totalement nulle sur un domaine fini. Nous avons donc défini la fonction coefficient des polynômes par :
\begin{verbatim}
Definition coef (p : polynomial) i := sub 0 p i.
\end{verbatim} 
La fonction \verb|coef| est de type \verb|nat -> R|. Étant donné un entier $i$, \verb|coef| retourne l'élément d'indice \verb|i| dans la séquence des coefficients du polynômes p si i est inférieur à la taille de cette séquence. Elle retourne 0 dans le cas contraire. \newline
Le lemme suivant permet d'avoir l'équivalence entre l'égalité entre les polynômes et celles entre les fonctions de coefficient : 
\begin{verbatim}
Lemma coef_eqP : forall p1 p2, coef p1 =1 coef p2 <-> p1 = p2.
\end{verbatim} 
Avec ce lemme, nous pouvons passer de notre représentation structurelle des polynômes vers celle qui ne considère que la fonction des coefficients. L'avantage de la second représentation est de rendre les preuves des propriétés algébriques des polynômes plus intuitives. Par exemple, la multiplication de deux polynômes est définie par :
\begin{equation}
\label{poly-mult}
\left(\sum_{i=0}^n a_ix^i\right)\left(\sum_{j=0}^m  b_jx^j\right)=\sum_{k=0}^{m+n}\left(\sum_{i +j =k}a_{i} b_{j}\right)x^k.
\end{equation} 
La preuve de l'associativité de cette multiplication se ramène à des raisonnements sur des sommes indexées, sans avoir besoin de faire des récurrences sur les polynômes.\newline
Dans la suite, les notations \verb|\X|, \verb|\C c| et \verb|\C0| correspondent respectivement au monôme $x$, au polynôme constant $c$ et au polynôme nul (la séquence vide).\newline
L'opération de multiplication d'un polynôme par \verb|x| (décalage à droite) et addition d'une constante est l'une des opérations de base sur les polynômes. Dans la librairie elle est réalisée par la fonction suivante : 
\begin{verbatim}
Definition horner c p : polynomial :=
  if p is Poly (Adds _ _ as s) ns then Poly (ns : normal (Adds c s)) else \C c.
\end{verbatim} 
A partir de cette définition, la fonction de construction d'un polynôme à partir d'une liste de coefficient se définit simplement par :
\begin{verbatim}
Definition mkPoly := foldr horner \C0.
\end{verbatim}
Les opérations de base sur les polynômes sont définies par récurrence sur la séquence des coefficients. La séquence résultat est ensuite normalisée par la fonction \verb|mkPoly|. Par exemple la multiplication de deux polynômes est définie comme suit :
\begin{verbatim}
Fixpoint mult_poly_seq (s1 s2 : seq R) {struct s1} : seq R :=
  if s1 is Adds c1 s1' then
    add_poly_seq (maps (fun c2 => c1 * c2) s2)
                 (Adds 0 (mult_poly_seq s1' s2))
  else seq0.

Definition mult_poly (p1 p2 : poly) := mkPoly (mult_poly_seq p1 p2).
\end{verbatim} 
Dans la seconde définition, la conversion de type entre les types \verb|polynomial| et \verb|seq| permet d'écrire \verb|mult_poly_seq p1 p2| bien que \verb|p1| et \verb|p2| sont de type \verb|polynomial|. Le lemme \verb|coef_mult_poly|
\begin{verbatim}
Lemma coef_mult_poly : forall p1 p2 k,
  coef (mult_poly p1 p2) k =
  \sum_(i : I_(k.+1)) (coef p1 i) * coef p2 (k - i).
\end{verbatim}
donne une relation entre les coefficients des deux polynômes et le résultat de leur multiplication. Il correspond à la relation de la formule (\ref{poly-mult}).
\paragraph{}
Une autre opération importante sur les polynômes est la fonction d'évaluation d'un polynôme. Elle consiste à remplacer sa variable par une valeur donnée. Cette fonction peut être décrite avec le schéma de Horner pour un polynôme \verb|p| et une valeur \verb|x| par :
\begin{equation}
 \label{horner-sch}
   p(x) = ((...((a_{n}x + a_{n-1})x + a_{n-2})x + ... ) + a_{1})x + a_{0}
\end{equation}
Suivant le schéma (\ref{horner-sch}) l'évaluation ne dépend que de la séquence des coefficients et se définit par récurrence sur cette séquence. La fonction d'évaluation peut être définie par récurrence comme suit :
\begin{verbatim}
Fixpoint eval_poly_seq (s : seq R) (x : R) {struct s} : R :=
  if s is (Adds a s') then eval_poly_seq s' x * x + a else 0.
Definition eval_poly (p : polynomial R) : R-> R := eval_poly_seq p.
\end{verbatim}
La notation \verb|p.[c]| correspond à l'application de la fonction \verb|eval_poly| en \verb|p| et \verb|c|. Les propriétés de morphisme de la fonction d'évaluation sont utilisées implicitement dans la preuve du théorème de Cayley-Hamilton. Ces propriétés sont données par les lemmes suivants :
\begin{verbatim}
Lemma eval_poly_plus : forall p q x,
 (p + q).[x] = p.[x] + q.[x].
Lemma eval_poly_mult : forall p q x,
 (forall i, (coef q i) * x = x * (coef q i)) ->
  (p * q).[x] = p.[x] * q.[x].
\end{verbatim} 
Dans le second lemme, la condition de commutativité entre les coefficients de \texttt{q} et \texttt{x} est nécessaire dans le cas d'un anneau non commutatif (les matrices par exemples).
\[
\mathrm{Soient} ~~~ p(x) = \sum_{i=0}^n a_ix^i ~~ \mathrm{et} ~~ q(x) = \sum_{j=0}^m  b_jx^j
\]
\begin{equation}
 \label{prod1}
\begin{array}{l l l l l }
  (p*q)(x) & = & \sum_{k=0}^{m+n}\left(\sum_{i+j =k}a_{i} b_{j}\right)x^k & = & \cdots a_{i} b_{j}x^(i +j) \cdots\\
\end{array} 
\end{equation} 
\begin{equation}
 \label{prod2}
\begin{array}{l l l l l }
  p(x)*q(x)& = & \left(\sum_{i=0}^n a_ix^i\right)\left(\sum_{j=0}^m  b_jx^j\right) & = & \cdots a_ix^ib_jx^j \cdots \\
\end{array} 
\end{equation}
La condition de commutativité s'explique donc par le fait que pour avoir (\ref{prod1}) égale à (\ref{prod2}), il faut que tous les $b_j$ commutent avec $x$.
\paragraph*{}
Après ces développements, le théorème du reste peut s'énoncer comme suit :
\begin{verbatim}
Theorem factor_poly : forall p c,
  (exists q, p = q * (\X - \C c)) <-> (p.[c] = 0).
\end{verbatim} 
Dans la preuve de ce théorème, pour pouvoir dire que \verb|p.[c]| est égale à \verb|q.[c] * (\X - \C c).[c]|, il faut prouver que les coefficients du polynôme \verb|(\X - \C c)| commutent avec \texttt{c}. Ce qui se prouve facilement car \texttt{1} et \texttt{c} commutent avec \texttt{c}. 

\subsection{Preuve de Cayley-Hamilton}
Le morphisme entre l'anneau des matrice de polynômes et celui des polynômes de matrices est la partie centrale de la preuve du théorème de Cayley-Hamilton. Les autres composantes de la preuve : la règle de Cramer et le théorème de factorisation, sont des propriétés qui se rattachent respectivement aux matrices et aux polynômes. \newline
Ce morphisme que nous allons appeler \verb|phi| prend en entrée une matrice de polynômes, lui applique le procédé décrit dans (\ref{morphism}), et retourne un polynôme de matrices. L'idée de l'algorithme pour construire ce morphisme est d'écrire la matrice de polynômes sous la forme d'une somme : $M + X M'$, avec $M$ une matrice sur l'anneau de base, $M'$ une matrice de polynômes et $X$ la matrice $xI_(n)$. Cette opération sera itérée autant de fois que la taille maximale des polynômes de la matrice de départ. La taille d'un polynôme correspond à la longueur de la séquence de ces coefficients, en d'autre terme son degré plus un. Dans la suite, les notations \verb|\Mp_(n)| et \verb|\Pm[x]| représentent respectivement l'anneau des matrices de polynômes et celui des polynômes de matrices. 
\begin{verbatim}
Definition phi (M : \Mp_(n)) : \Pm[x] :=
  foldr (fun k p => 
             (horner p (matrix_of_fun (fun i j=> coef (M i j) k))))
  \C0 (iota 0 (size_mx_of_poly M)).
\end{verbatim} 
Pour définir \verb|phi|, nous passons à \verb|foldr| la fonction qui étant donnée une matrice de polynômes \verb|M|, un polynôme de matrices \verb|p| et un indice \verb|k|, décale le polynôme \verb|p| à droite et lui ajoute comme coefficient de plus bas degré la matrice des coefficients d'indices \verb|k| des polynômes de la matrices \verb|M|. La fonction \verb|iota a b| construit une séquence d'entiers commencant par \texttt{a} et de longueur \texttt{b} ou une séquence vide si \texttt{b} est nulle.\newline
Par ailleurs il est aussi nécessaire d'avoir une relation entre le résultat de \verb|phi| et sa valeur d'entrée. La propriété de certification de la fonction \verb|phi| est la suivante :
\begin{verbatim}
Lemma phi_coef : forall (M : \Mp_(n)) i j k,
  coef (M i j) k = (coef (phi M) k) i j.
\end{verbatim} 
Ce lemme dit que pour toute matrice de polynômes $M$, le coefficient du polynôme $M_{i, j}$ en $k$ est égale à l'élément d'indice $(i, j)$ de la matrice coefficient en $k$ de l'image de $M$ par \verb|phi|.
\paragraph*{}
Pour pouvoir définir l'évaluation d'une matrice en son polynôme caractéristique, nous avons défini l'injection de l'anneau des polynômes vers celui des polynômes de matrices. Cette fonction prend un polynôme sur un anneau de base $R$ et multiplie ses coefficients par la matrice identité pour obtenir un polynôme à coefficients matriciels. La notation \texttt{p2pm} correspond à cette fonction. Après ces définitions, le théorème de Cayley-Hamilton s'énonce formellement de la façon suivante :
\begin{verbatim}
Theorem Cayley_Hamilton : forall A, (p2pm (poly_car A)).[A] = 0.
\end{verbatim} 

La preuve se déroule exactement comme décrit dans la seconde section. Après avoir généralisé la règle de Cramer, nous lui appliquons  le morphisme \verb|phi|. Le résultat du théorème de Cayley-Hamilton est alors obtenu par des réécritures et simplifications dans le terme obtenu.

\section{Conclusion}
Nous avons présenté une formalisation du théorème de Cayley-Hamilton qui adopte une approche modulaire. La preuve que nous avons présenté dans la section 4.4 peut paraître très simple; mais la conception a été assez longue que ce soit pour choisir l'architecture globale de la preuve ou le bon type de données pour représenter les structures manipulées (polynômes et matrices). Ceci a été d'autant plus difficile car dans la preuve les structures utilisées vont être combinées les unes avec les autres. Les choix ont été motivés par des soucis de lisibilité et de réutilisabilité. L'utilisation des {Canonical Structure} de \coq{} nous a permis d'avoir des énoncés proches de ceux utilisés en mathématiques classiques. Le découpage des différentes composantes de la preuve sous forme modulaire ( les opérations indexes, les matrices et les polynômes) favorise la réutilisation de ces librairies dans des développements indépendants. Les librairies sur les opérations indexées et les matrices seront réutilisées dans nos prochains travaux sur la théories des caractères, une des composantes de la preuves du théorème de Feit-Thompson.\newline
Ce travail que nous avons présenté ici est la première formalisation du théorème de Cayley-Hamilton. Ce n'est pas la première formalisation des matrices ou des polynômes. Des formalisations de ces structures sont présentées respectivement dans~\cite{ring-mx, linalg} et~\cite{fta, factor-th}. Mais c'est le premier développement qui regroupe une formalisation des matrices et polynômes et où ces structures sont assemblées pour former de nouvelle structure : les matrices de polynômes et les polynômes de matrices. Dans ce développement la librairie sur les polynômes comprend 83 objets (définitions et lemmes) pour environs 490 lignes de codes. Celle pour les matrices comprend 89 objets pour 700 lignes de codes. Les définitions et lemmes propres à la preuve du théorème de Cayley-Hamilton sont au nombre de 15 pour 125 lignes de codes. Les sources du développement sont disponibles à l'adresse suivante : \verb|http://www-sop.inria.fr/marelle/Sidi.Biha/cayley/|.  
\paragraph*{}
Dans la formalisation du théorème de Cayley-Hamilton, présentée dans cet article, nous avons choisi de construire nos structures de données sur des types munis d'une égalité décidables: les \texttt{eqType}. En plus du fait qu'en mathématiques classiques tous les types sont décidables, notre preuve sur les types décidables peut être généralisés vers les types quelconques. Ceci se fait en remarquant que tout anneau est une $\mathbf{Z}$-algèbre et en considérant le morphisme d'évaluation des polynômes à $n^{2}$ variables et à coefficients dans $\mathbf{Z}$ qui est un type décidable. Puisque les opérations algébriques sur les matrices et les polynômes n'agissent qu'en fonction des indices des coefficients, ce morphisme commute avec ces opérations. Par conséquent le théorème de Cayley-Hamilton sur les types décidables peut être généralisé à des types quelconques. Cette démarche n'est pas difficile mais elle est fastidieuse et d'aucune utilité pour le travail que nous faisons dans le cadre du projet ``Mathematical Components''. L'avantage de construire nos structures de données (anneaux, matrices et polynômes) sur des types décidables est d'avoir une égalité de Leibniz sur ces structures. Nous pourrons alors profiter de la puissance de la réécriture avec cette égalité, qui est la règle de réécriture par défaut dans \coq{}.

% La bibliographie
\begin{thebibliography}{10} % N'oubliez pas de l'inclure lors
\bibitem{primeth}
  Jeremy \textsc{Avigad}, Kevin \textsc{Donnelly}, David \textsc{Gray}, et Paul \textsc{Raff},
  \textit{A Formally Verified Proof of the Prime Number Theorem},
  ACM Transactions on Computational Logic, A paraître.

\bibitem{coqart}
  Yves \textsc{Bertot}, Pierre \textsc{Castéran},
  \textit{Interactive Theorem Proving and Program Development Coq'Art: The Calculus of Inductive Constructions},
  Springer Verlag,
  2004.

\bibitem{algebra}
  Nathan \textsc{Jacobson},
  \textit{Lectures in Abstract Algebra: II. Linear Algebra},
  Springer Verlag, 1975.

\bibitem{fta}
  Herman \textsc{Geuvers}, Freek \textsc{Wiedijk} et Jan \textsc{Zwanenburg},
  \textit{A Constructive Proof of the Fundamental Theorem of Algebra without Using the Rationals},
  Types for Proofs and Programs, TYPES 2000 International Workshop, Selected Papers, volume 2277 of
  LNCS, pages 96-111, 2002.

\bibitem{4colproof}
  Georges \textsc{Gonthier},
  \textit{A computer-checked proof of the four-colour theorem},
  Disponible à http://research.microsoft.com/~gonthier/4colproof.pdf.

\bibitem{not4col}
  Georges \textsc{Gonthier},
  \textit{Notations of the four colour theorem proof},
  Disponible à http://research.microsoft.com/~gonthier/4colnotations.pdf.

\bibitem{modgrp}
  Georges \textsc{Gonthier}, Assia \textsc{Mahboubi}, Laurence \textsc{Rideau}, Enrico \textsc{Tassi} et Laurent \textsc{Théry},
  \textit{A Modular Formalisation of Finite Group Theory},
  Rapport de Recherche 6156, INRIA, 2007.

\bibitem{manif}
  Georges \textsc{Gonthier}, Benjamin \textsc{Werner}, Yves \textsc{Bertot},
  \textit{Mathematical Components Manifesto},
  Disponible à http://www.msr-inria.inria.fr/projects/math/manifesto.html.

\bibitem{ring-mx}
  Nicolas \textsc{Magaud},
  \textit{Ring properties for square matrices} contribution à Coq,
  http://coq.inria.fr/contribs-eng.html.

\bibitem{factor-th}
  Piotr \textsc{Rudnicki},
  \textit{Little Bezout Theorem (Factor Theorem)}, Journal of Formalized Mathematics volume 15, 2003,
  Disponible à http://mizar.org/JFM/Vol15/uproots.html.

\bibitem{linalg}
  Jasper \textsc{Stein},
  \textit{Linear Algebra} contribution à Coq,
  http://coq.inria.fr/contribs-eng.html.

\bibitem{coqman}
  \textsc{Coq Team},
  \textit{The Coq reference manual V 8.1},
  http://coq.inria.fr/V8.1/refman/index.html.

\bibitem{100th}
  Freek \textsc{Wiedijk},
  \textit{Formalizing 100 Theorems},
  http://www.cs.ru.nl/freek/100/.

\end{thebibliography}

\pagebreak
\thispagestyle{colloquetitle}
\cleardoublepage
\end{document}